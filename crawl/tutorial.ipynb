{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 크롤링 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 requests fake-useragent selenium chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 페이지를 불러와서 추출 가능한 상태로 만들기\n",
    "\n",
    "정적 데이터의 경우 requests의 get만 활용해도 되고 동적 데이터의 경우 selenium의 get를 활용하면 된다.\n",
    "\n",
    "도전해보고싶은 페이지가 있는 경우 그 페이지를 활용하고, 없으면 다음 예시를 사용한다.\n",
    "\n",
    "리스트 페이지는 https://ridibooks.com/category/books/2200?order=recent\n",
    "\n",
    "각 데이터 페이지의 예시는 https://ridibooks.com/books/4489000001\n",
    "\n",
    "각 페이지에서 제목, 저자, 전자책 출간일, 판매가, 평점, 평점 남긴 사람의 수를 크롤링하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_selenium_driver(options=None):\n",
    "  \"\"\"\n",
    "  이 함수는 Selenium driver를 리턴하는 함수이다.\n",
    "  \"\"\"\n",
    "  from selenium import webdriver\n",
    "  import chromedriver_autoinstaller\n",
    "  from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "  chrome_path = chromedriver_autoinstaller.install(True)\n",
    "  s = Service(chrome_path)\n",
    "\n",
    "  return webdriver.Chrome(\n",
    "    service=s, # 서비스\n",
    "    options=options\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 다른 것을 하고싶다면 그 페이지를 변수에 넣어보자.\n",
    "SINGLE_PAGE_URL = \"https://ridibooks.com/books/4489000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# TODO: BeautifulSoup를 사용하거나 Selenium를 사용해서 페이지를 데이터 추출 가능 상태로 만든다.\n",
    "\n",
    "def load_single_page_bs4(url):\n",
    "  \"\"\"\n",
    "  TODO: 페이지의 URL을 인수로 받아 Soup를 리턴하는 함수를 작성해봅시다.\n",
    "  requests를 사용하는 경우 User Agent에 신경쓰면서 GET을 해봅시다.\n",
    "  \"\"\"\n",
    "  pass\n",
    "\n",
    "\n",
    "def load_single_page_selenium(url, driver=None):\n",
    "  \"\"\"\n",
    "  TODO: 페이지의 URL을 인수로 받아 Selenium driver에 GET을 동작시키는 함수를 작성해봅시다.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 테스트\n",
    "\n",
    "# BeautifulSoup\n",
    "## data_soup = load_single_page_bs4(SINGLE_PAGE_URL)\n",
    "\n",
    "# Selenium\n",
    "## driver = load_selenium_driver()\n",
    "## load_single_page_selenium(SINGLE_PAGE_URL, driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 데이터를 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS Selector, XPath, BeautifulSoup 문법을 사용해서 원하는 데이터까지 경로를 이용해 데이터를 추출한다\n",
    "\n",
    "def extract_data_bs4(soup):\n",
    "  \"\"\"\n",
    "  TODO: 이 함수는 Soup를 인수로 받아 원하는 데이터들의 Dictionary를 리턴하는 함수입니다.\n",
    "  \"\"\"\n",
    "  return { \"foo\": \"bar\" }\n",
    "\n",
    "\n",
    "def extract_data_selenium(driver):\n",
    "  \"\"\"\n",
    "  TODO: 이 함수는 Selenium을 인수로 받아 원하는 데이터들의 Dictionary를 리턴하는 함수입니다.\n",
    "  \"\"\"\n",
    "  return { \"foo\": \"bar\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 테스트\n",
    "\n",
    "# BeautifulSoup\n",
    "## bs4_data = extract_data_bs4(data_soup)\n",
    "## print (\"Soup:\", bs4_data)\n",
    "\n",
    "# Selenium\n",
    "## selenium_data = extract_data_selenium(driver)\n",
    "## print (\"Selenium:\", selenium_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 원하는 곳에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Client를 구하는 함수\n",
    "import pymongo\n",
    "\n",
    "def get_mongo_client():\n",
    "  URI = f'mongodb://username:password@localhost:27017'\n",
    "  return pymongo.MongoClient(URI)\n",
    "\n",
    "def get_table_from_client(client: pymongo.MongoClient):\n",
    "  return client['crawl_test_db']['crawl_test_tb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘 정비되있고 크롤링이 오래 안걸리는 데이터면 pandas같은 것을 통해 CSV 파일로 적당히 저장해도 좋지만\n",
    "# 그게 아니라면 관리가 가능하면서 사용이 어렵지 않은 MongoDB에 저장한다\n",
    "\n",
    "def save_data(data):\n",
    "  \"\"\"\n",
    "  TODO: 이 함수는 데이터를 원하는 장소에 저장하는 함수입니다. 원하는 곳에 저장하되, 여러 데이터가 이어서 저장될 수 있도록 합시다.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 테스트\n",
    "\n",
    "# BeautifulSoup\n",
    "## save_data(bs4_data)\n",
    "\n",
    "# Selenium\n",
    "## save_data(selenium_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리스트 페이지에서 각 페이지 위치 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 다른 것을 하고싶다면 그 리스트 페이지를 변수에 넣어보자.\n",
    "LIST_FIRST_PAGE_URL = \"https://ridibooks.com/category/books/2200?order=recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup4의 경우에는 리스트 페이지에서 각 하위 페이지의 URL을 구하고,\n",
    "# Selenium의 경우에는 리스트 페이지에서 각 하위 페이지의 URL을 구하거나 element를 구하면 된다.\n",
    "# 다음 과정에서는 각 URL을 통해 그 페이지로 이동하거나, element를 클릭하여 다음 페이지로 이동할 수 있다.\n",
    "\n",
    "def get_all_single_page_url_bs4(list_page_soup):\n",
    "  \"\"\"\n",
    "  TODO: 리스트 페이지에서 Selector 등을 활용하여 모든 하위 페이지의 href를 구해봅시다.\n",
    "  \"\"\"\n",
    "  pass\n",
    "\n",
    "\n",
    "def get_all_single_page_elem_selenium(driver):\n",
    "  \"\"\"\n",
    "  TODO: 리스트 페이지에서 Selector 등을 활용하여 모든 하위 페이지의 elements들을 구해봅시다.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 테스트\n",
    "\n",
    "# BeautifulSoup\n",
    "## list_page_soup = load_single_page_bs4(LIST_FIRST_PAGE_URL)\n",
    "## print(get_all_single_page_url_bs4(list_page_soup))\n",
    "\n",
    "\n",
    "# Selenium\n",
    "## load_single_page_selenium(LIST_FIRST_PAGE_URL, driver)\n",
    "## driver.implicitly_wait(5)\n",
    "## single_pages = get_all_single_page_elem_selenium(driver)\n",
    "## for x in single_pages: print(x.get_attribute('class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 큰 페이지에서 작은 페이지를 순서대로 크롤링하여 모든 과정 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def crawl_bs4(list_page_url):\n",
    "  \"\"\"\n",
    "  TODO: \n",
    "  \"\"\"\n",
    "  next_page_url = list_page_url\n",
    "  while next_page_url != None:\n",
    "    # 리스트 페이지를 로드하고\n",
    "    list_page_soup = load_single_page_bs4(next_page_url)\n",
    "\n",
    "    # 리스트 페이지에서 모든 데이터 페이지의 URL을 구한다.\n",
    "    all_single_page_url = get_all_single_page_url_bs4(list_page_soup)\n",
    "\n",
    "    # 각 페이지의 URL에 대해\n",
    "    for single_page_url in all_single_page_url:\n",
    "      try:\n",
    "        # 각 페이지를 로드해서\n",
    "        single_page_soup = load_single_page_bs4(single_page_url)\n",
    "\n",
    "        # 데이터를 뽑고\n",
    "        data = extract_data_bs4(single_page_soup)\n",
    "\n",
    "        # 저장하고\n",
    "        save_data(data)\n",
    "\n",
    "        # 서버에 부하를 주지않기 위해 Delay를 준 후 반복한다.\n",
    "        sleep(3)\n",
    "\n",
    "      except Exception as e:\n",
    "        print (\"Error occurred while running crawling with BeautifulSoup:\", e)\n",
    "        break\n",
    "\n",
    "    # TODO: 다음 URL이 있으면 다음 페이지로 이동한다.\n",
    "    next_page_url = None # get_next_url_bs4(list_page_soup)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "def crawl_selenium(list_page_url):\n",
    "  crawl_driver = load_selenium_driver()\n",
    "  # 리스트 페이지를 로드한다.\n",
    "  load_single_page_selenium(crawl_driver, list_page_url)\n",
    "\n",
    "  while True:\n",
    "    crawl_driver.implicitly_wait(5)\n",
    "\n",
    "    # 각 리스트 페이지에서 모든 데이터 페이지의 URL을 구한다.\n",
    "    # 각 elements에 대해\n",
    "    for data_page_element in get_all_single_page_elem_selenium(crawl_driver):\n",
    "      try:\n",
    "        # TODO: 적당히 각 페이지를 로드해서\n",
    "        data_page_element.click()\n",
    "\n",
    "        # 데이터를 뽑고\n",
    "        data = extract_data_selenium(crawl_driver)\n",
    "\n",
    "        # 저장하고\n",
    "        save_data(data)\n",
    "\n",
    "        # 다시 리스트 페이지로 나온다\n",
    "        crawl_driver.back()\n",
    "\n",
    "        # 서버에 부하를 주지않기 위해 Delay를 준 후 반복한다.\n",
    "        sleep(3)\n",
    "\n",
    "      except Exception as e:\n",
    "        print (\"Error occurred while running crawling with selenium:\", e)\n",
    "        break\n",
    "\n",
    "    # TODO: 다음 URL이 있으면 다음 페이지로 이동한다.\n",
    "    break\n",
    "\n",
    "  crawl_driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 테스트\n",
    "\n",
    "# BeautifulSoup\n",
    "## crawl_bs4(LIST_FIRST_PAGE_URL)\n",
    "\n",
    "# Selenium\n",
    "## crawl_selenium(LIST_FIRST_PAGE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임의의 시간에 자동으로 실행되게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리눅스, 맥의 경우는 crontab을 이용하고 윈도우즈의 경우에는 스케쥴러를 이용한다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
